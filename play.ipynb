{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T13:24:41.608783Z",
     "start_time": "2025-11-18T13:24:36.046648Z"
    }
   },
   "source": "from transformers import GPT2LMHeadModel",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\python\\conda\\envs\\lightning_learn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T13:31:50.911651Z",
     "start_time": "2025-11-18T13:25:24.172915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_hp=GPT2LMHeadModel.from_pretrained('gpt2')#124M版本\n",
    "# model_hp=GPT2LMHeadModel.from_pretrained('gpt2-xl')#15B版本\n",
    "# 获取state_dict,不要模型结构\n",
    "# sd_hp是模型参数字典\n",
    "sd_hp=model_hp.state_dict()\n",
    "\n",
    "for k,v in sd_hp.items():\n",
    "    # 输出dict的key和value\n",
    "    print(k,v.shape)"
   ],
   "id": "77a80217321edd8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\python\\conda\\envs\\lightning_learn\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T13:56:25.513103Z",
     "start_time": "2025-11-18T13:56:25.496281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transformer.wte.weight torch.Size([50257, 768])   weight token embedding\n",
    "# transformer.wpe.weight torch.Size([1024, 768])    weight position embedding 位置矩阵,gpt2最大输入长度为1024token"
   ],
   "id": "73dc3988619006c5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T13:58:05.216526Z",
     "start_time": "2025-11-18T13:57:49.980709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline,set_seed\n",
    "# using pipeline\n",
    "generator=pipeline('text-generation',model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"hello world\",max_length=30,num_return_sequences=5)"
   ],
   "id": "4a445fb7fc2b449e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"hello world, where the world's population is growing and where there are all the other ways people work and live. It's a pretty big deal. All of this stuff that's happening in this country is in the open. It's a big deal, especially here in Colorado.\\n\\nAnd then the other side of the coin, the way that it's happening, is that people are not going to be able to get on the bus or any of those other ways of connecting with things that are happening in this country. And that's a very sad fact. Because it's a lot easier to live and work in this country than you are in this country. And that's why, for me, this is a good time of year. I can say that in 2014 the economy is booming and I'm looking forward to going to work every day.\\n\\nQ: Are you going to take a vacation?\\n\\nA: No, I don't. I'm just going to sit here and talk about my life and about my family.\\n\\nQ: I'm going to stop by your family.\\n\\nA: Yeah.\\n\\nQ: Hi, I'm going to see you for a couple of days.\\n\\nA: Yeah, I feel comfortable coming over\"},\n",
       " {'generated_text': 'hello world.\\n\\nThe World Bank has released a report of its own that shows that China\\'s \"unprecedented\" growth rate is overstated.\\n\\nThe report stated that the current rate is \"more than twice the current level\" and \"almost ten times the current level in real terms.\"\\n\\nThe report said that China\\'s growth rate is \"far above or below\" the US growth rate.\\n\\nThe report said the average real wages of Chinese workers have fallen by over 10 percent since 2009.\\n\\nThe report said the government has been able to expand its economic and trade base to bring in around 200 million workers.\\n\\nThe report noted that \"the economic development plan for China showed that it is already taking advantage of the strengths of large-scale trade, investment and investment banking to generate economic growth.\"\\n\\nChina\\'s GDP per head is currently 3.3 percent, the highest in the world.\\n\\nThe report said the government should not be surprised that China\\'s GDP per head is currently on a steady downward trend.\\n\\nThe report said the government should try to raise the minimum wage to pay for the construction of new roads and bridges.\\n\\nChina\\'s GDP per capita is 2.6 percent higher than the US.\\n\\nThe report'},\n",
       " {'generated_text': 'hello world, and the one place where the only thing you can be sure about is you\\'re not going to give up, it seems you\\'re going to let your life be like, \"That\\'s my life!\"\\n\\nThen there\\'s the \"other\" side of \"the world.\" It\\'s a strange place. The world that\\'s being constructed is not the world we\\'ve just created. The world we\\'re living in isn\\'t the world we\\'ve just created. It\\'s not the world that you would know about. It really is a world that you\\'re not going to be interested in. You\\'re not going to be interested in the world.\\n\\nIt\\'s interesting that there\\'s a lot of people who haven\\'t really taken the time to understand the world. I\\'m not talking about the people living in it. I\\'m talking about people who are living in it, and I\\'ll let you go through some of the things that happened. But it\\'s really interesting. And it\\'s really interesting to think about.\\n\\nThe \"other\" side of the globe, the \"other\" side of the world, is the world we\\'ve just created, and it\\'s not the world we\\'re living in. We\\'ve just created \"the other\" planet. I remember the'},\n",
       " {'generated_text': 'hello world.\\n\\nHindsight is 20/20 for what it is:\\n\\nAs we have seen, the internet is changing and we are getting more and more of things. While the internet is the main thing in the world today, people are also making more and more use of their digital lives. We are not just a digital world, we are a real one. We are changing the world through the internet, not just through physical means.\\n\\nThe internet is the most important thing in the world today. The internet has changed the way we interact with the world. I hope you will see what I am talking about, and there is no doubt that the internet is changing the way you interact with the world. The internet is changing you, not just by creating the internet, but also by creating the internet itself.\\n\\nThe internet is changing you in a number of ways.\\n\\nYou can start using the internet. You can start using the internet. The internet, like all things, is changing you. Your internet connection is changing you. Your internet connection is changing you. Your internet connection is changing you.\\n\\nYou can use the internet to learn. You can use the internet to learn what is going on in your life. You can use the'},\n",
       " {'generated_text': 'hello world that we live in.\\n\\nHe then asked if he could bring a book on the subject.\\n\\n\"Well, I do have a book on the subject, and I\\'ll be sure to bring it along,\" he said.\\n\\nHe said he had been inspired by the \"mysterious girl from the Twilight Zone\" in the Twilight Zone series.\\n\\n\"I think it\\'s really important for me to tell that story about the story of a girl and her journey to the end,\" he said.\\n\\n\"She\\'s a really, really beautiful girl, and she\\'s not just a girl in a show, she\\'s a girl who is a girl who lives in a world where she\\'s not only an end to a story but a starting point for much greater things.\"\\n\\nHis book is due out in early 2018.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T12:47:33.038056Z",
     "start_time": "2025-11-19T12:47:33.015022Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "20d1333777770807",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T12:50:05.786729Z",
     "start_time": "2025-11-19T12:50:05.774156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "with open('input.txt','r',encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "    # print(text[:1000])\n",
    "encoding=tiktoken.get_encoding('gpt2')\n",
    "data=encoding.encode(text[:1000])\n",
    "print(len(data),data[:10])"
   ],
   "id": "ebf95cd814ff96be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T13:01:43.031943Z",
     "start_time": "2025-11-19T13:01:43.021434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "buf=torch.tensor(data[:24+1])\n",
    "# 通过这个buffer来观察部分输入和输出\n",
    "# 自回归的输入和输出只差一个时间位置\n",
    "x=buf[:-1].view(4,6)\n",
    "y=buf[1:].view(4,6)\n",
    "x,y"
   ],
   "id": "6f1178ef1103e4dd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "         [ 5120,   597,  2252,    11,  3285,   502],\n",
       "         [ 2740,    13,   198,   198,  3237,    25],\n",
       "         [  198,  5248,   461,    11,  2740,    13]]),\n",
       " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
       "         [  597,  2252,    11,  3285,   502,  2740],\n",
       "         [   13,   198,   198,  3237,    25,   198],\n",
       "         [ 5248,   461,    11,  2740,    13,   198]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d17f3f28c1a5f5a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
